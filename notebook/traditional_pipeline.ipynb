{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO-zFedG-T4d"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXBXV4y_-OYH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import warnings\n",
        "\n",
        "nltk.download('stopwords')\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baGDx7g2-Yux"
      },
      "source": [
        "# Text Preprocessing Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCoWJZPU-W1r"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, stopwords):\n",
        "        self.stopwords = stopwords\n",
        "        self.punctuation = re.compile(r'[^\\w\\s]')\n",
        "\n",
        "    def clean_text(self, text, for_bert=False):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "        text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        if not for_bert:\n",
        "            text = self.punctuation.sub('', text)\n",
        "            text = ' '.join([w for w in text.split() if w not in self.stopwords])\n",
        "        return text\n",
        "\n",
        "    def save(self, path):\n",
        "        joblib.dump(self, path)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        return joblib.load(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7JWEcj-x2_"
      },
      "source": [
        "# SVM Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDUGeqOg-yGd"
      },
      "outputs": [],
      "source": [
        "class ABSASVM:\n",
        "    def __init__(self, aspects):\n",
        "        self.aspects = aspects\n",
        "        self.models = {}\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=10000,\n",
        "            ngram_range=(1, 2),\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train_dict, X_val, y_val_dict):\n",
        "        X_train_vec = self.vectorizer.fit_transform(X_train)\n",
        "        X_val_vec = self.vectorizer.transform(X_val)\n",
        "\n",
        "        for aspect in self.aspects:\n",
        "            print(f\"\\nTraining SVM for {aspect}\".ljust(50, '-'))\n",
        "            svm = SVC(\n",
        "                kernel='rbf',\n",
        "                class_weight='balanced',\n",
        "                C=1.0,\n",
        "                gamma='scale',\n",
        "                random_state=42\n",
        "            )\n",
        "            svm.fit(X_train_vec, y_train_dict[aspect])\n",
        "\n",
        "            val_preds = svm.predict(X_val_vec)\n",
        "            val_acc = accuracy_score(y_val_dict[aspect], val_preds)\n",
        "            print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "            self.models[aspect] = svm\n",
        "\n",
        "    def save(self, model_dir):\n",
        "        model_dir = Path(model_dir)\n",
        "        model_dir.mkdir(parents=True, exist_ok=True)\n",
        "        joblib.dump(self.vectorizer, model_dir/\"vectorizer.joblib\")\n",
        "        for aspect in self.aspects:\n",
        "            joblib.dump(self.models[aspect], model_dir/f\"{aspect}.joblib\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, model_dir, aspects):\n",
        "        model_dir = Path(model_dir)\n",
        "        instance = cls(aspects)\n",
        "        instance.vectorizer = joblib.load(model_dir/\"vectorizer.joblib\")\n",
        "        instance.models = {a: joblib.load(model_dir/f\"{a}.joblib\") for a in aspects}\n",
        "        return instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2cMaS1Z-7DW"
      },
      "source": [
        "# BERT Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-PTqXuL-9ks"
      },
      "outputs": [],
      "source": [
        "class ABSABERT:\n",
        "    def __init__(self, aspects, model_name='indolem/indobert-base-uncased'):\n",
        "        self.aspects = aspects\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.models = {\n",
        "            a: AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "            for a in aspects\n",
        "        }\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def train(self, train_loader_dict, val_loader_dict, epochs=10, lr=2e-5):\n",
        "        scaler = GradScaler()  # For mixed precision\n",
        "        for aspect in self.aspects:\n",
        "            print(f\"\\nTraining BERT for {aspect}\".ljust(50, '-'))\n",
        "            model = self.models[aspect].to(self.device)\n",
        "            optimizer = AdamW(model.parameters(), lr=lr)\n",
        "            scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=0,\n",
        "                num_training_steps=len(train_loader_dict[aspect]) * epochs\n",
        "            )\n",
        "\n",
        "            best_loss = float('inf')\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "                total_loss = 0\n",
        "                for batch in train_loader_dict[aspect]:\n",
        "                    batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    with autocast():  # Mixed precision\n",
        "                        outputs = model(**batch)\n",
        "                        loss = outputs.loss\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    scheduler.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                avg_loss = total_loss / len(train_loader_dict[aspect])\n",
        "                # Validation step\n",
        "                model.eval()\n",
        "                val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_loader_dict[aspect]:\n",
        "                        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                        with autocast():\n",
        "                            outputs = model(**batch)\n",
        "                        val_loss += outputs.loss.item()\n",
        "                avg_val_loss = val_loss / len(val_loader_dict[aspect])\n",
        "                print(f\"Epoch {epoch+1}: Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                if avg_val_loss < best_loss:\n",
        "                    best_loss = avg_val_loss\n",
        "                    torch.save(model.state_dict(), f\"best_{aspect}.pt\")\n",
        "            model.load_state_dict(torch.load(f\"best_{aspect}.pt\"))\n",
        "\n",
        "\n",
        "    def save(self, model_dir):\n",
        "        model_dir = Path(model_dir)\n",
        "        model_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.tokenizer.save_pretrained(model_dir)\n",
        "        for aspect in self.aspects:\n",
        "            aspect_dir = model_dir/f\"{aspect}\"\n",
        "            aspect_dir.mkdir(exist_ok=True)\n",
        "            self.models[aspect].save_pretrained(aspect_dir)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, model_dir, aspects):\n",
        "        model_dir = Path(model_dir)\n",
        "        instance = cls(aspects, model_name=model_dir)\n",
        "        instance.models = {\n",
        "            a: AutoModelForSequenceClassification.from_pretrained(model_dir/a)\n",
        "            for a in aspects\n",
        "        }\n",
        "        instance.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        return instance\n",
        "\n",
        "# Dataset class for BERT\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6KKqrKM_sGN"
      },
      "source": [
        "# Inference Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or-JucoB_uLf"
      },
      "outputs": [],
      "source": [
        "class ABSAInferencer:\n",
        "    def __init__(self, model_dir, aspects, preprocessor, model_type='bert'):\n",
        "        self.aspects = aspects\n",
        "        self.preprocessor = preprocessor\n",
        "        self.model_type = model_type\n",
        "        self.label_map = {0: 'neg', 1: 'neut', 2: 'pos'}\n",
        "\n",
        "        if model_type == 'bert':\n",
        "            self.model = ABSABERT.load(Path(model_dir), aspects)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(Path(model_dir))\n",
        "        else:\n",
        "            self.model = ABSASVM.load(Path(model_dir), aspects)\n",
        "\n",
        "    def predict(self, text):\n",
        "        if self.model_type == 'bert':\n",
        "            return self._predict_bert(text)\n",
        "        return self._predict_svm(text)\n",
        "\n",
        "    def _predict_bert(self, text):\n",
        "        clean_text = self.preprocessor.clean_text(text, for_bert=True)\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            clean_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        preds = {}\n",
        "        for aspect in self.aspects:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.models[aspect](**encoding)\n",
        "            preds[aspect] = self.label_map[torch.argmax(outputs.logits).item()]\n",
        "        return preds\n",
        "\n",
        "    def _predict_svm(self, text):\n",
        "        clean_text = self.preprocessor.clean_text(text, for_bert=False)\n",
        "        X = self.model.vectorizer.transform([clean_text])\n",
        "        return {\n",
        "            aspect: self.label_map[self.model.models[aspect].predict(X)[0]]\n",
        "            for aspect in self.aspects\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ1Le19-_1Cy"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O2FvbmF_29q"
      },
      "outputs": [],
      "source": [
        "def load_and_convert_data(file_path, aspect_cols):\n",
        "    \"\"\"Load CSV and convert sentiment labels to integers\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    label_map = {'neg': 0, 'neut': 1, 'pos': 2}\n",
        "\n",
        "    for col in aspect_cols:\n",
        "        df[col] = df[col].map(label_map).fillna(1).astype('int8')  # neut as default\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Training\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    aspects = ['ac', 'air_panas', 'bau', 'general', 'kebersihan',\n",
        "             'linen', 'service', 'sunrise_meal', 'tv', 'wifi']\n",
        "\n",
        "    df_train = load_and_convert_data('train_preprocess.csv', aspects)\n",
        "    df_val = load_and_convert_data('valid_preprocess.csv', aspects)\n",
        "\n",
        "    # Initialize components\n",
        "    stopwords_id = stopwords.words('indonesian')\n",
        "    preprocessor = TextPreprocessor(stopwords_id)\n",
        "\n",
        "    # Preprocess text\n",
        "    for df in [df_train, df_val]:\n",
        "        df['clean_svm'] = df['review'].apply(preprocessor.clean_text, for_bert=False)\n",
        "        df['clean_bert'] = df['review'].apply(preprocessor.clean_text, for_bert=True)\n",
        "\n",
        "    # Train SVM\n",
        "    svm_model = ABSASVM(aspects)\n",
        "    svm_model.train(\n",
        "        df_train['clean_svm'],\n",
        "        {a: df_train[a] for a in aspects},\n",
        "        df_val['clean_svm'],\n",
        "        {a: df_val[a] for a in aspects}\n",
        "    )\n",
        "\n",
        "    # Train BERT\n",
        "    bert_model = ABSABERT(aspects)\n",
        "    train_loaders = {\n",
        "        a: DataLoader(\n",
        "            ABSADataset(df_train['clean_bert'], df_train[a], bert_model.tokenizer),\n",
        "            batch_size=16,\n",
        "            shuffle=True\n",
        "        ) for a in aspects\n",
        "    }\n",
        "    val_loaders = {\n",
        "        a: DataLoader(\n",
        "            ABSADataset(df_val['clean_bert'], df_val[a], bert_model.tokenizer),\n",
        "            batch_size=16\n",
        "        ) for a in aspects\n",
        "    }\n",
        "    bert_model.train(train_loaders, val_loaders)\n",
        "\n",
        "    # Save models\n",
        "    model_dir = Path(\"saved_models\")\n",
        "    svm_model.save(model_dir/\"svm\")\n",
        "    bert_model.save(model_dir/\"bert\")\n",
        "    preprocessor.save(model_dir/\"preprocessor.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B4j40rXAOwC"
      },
      "source": [
        "Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfXMM5-JAQ20"
      },
      "outputs": [],
      "source": [
        "test_text = \"lumayan nyaman,tp kebersihan kmr mandi perlu ditingkatkan lg biar gk ada kuning2 di sudutnya lbh bgs\"\n",
        "\n",
        "preprocessor = TextPreprocessor.load(model_dir/\"preprocessor.joblib\")\n",
        "svm_infer = ABSAInferencer(model_dir/\"svm\", aspects, preprocessor, 'svm')\n",
        "bert_infer = ABSAInferencer(model_dir/\"bert\", aspects, preprocessor, 'bert')\n",
        "print(\"\\nSVM Predictions:\", svm_infer.predict(test_text))\n",
        "print(\"\\nBERT Predictions:\", bert_infer.predict(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCUWPg17Abt3"
      },
      "source": [
        "# Test Data Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhpNPcMdAj3m"
      },
      "outputs": [],
      "source": [
        "df_test = load_and_convert_data('test_preprocess.csv', aspects)\n",
        "aspects = ['ac', 'air_panas', 'bau', 'general', 'kebersihan',\n",
        "           'linen', 'service', 'sunrise_meal', 'tv', 'wifi']\n",
        "\n",
        "# Load inferencers\n",
        "preprocessor = TextPreprocessor.load(model_dir/\"preprocessor.joblib\")\n",
        "svm_infer = ABSAInferencer(model_dir/\"svm\", aspects, preprocessor, 'svm')\n",
        "bert_infer = ABSAInferencer(model_dir/\"bert\", aspects, preprocessor, 'bert')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fO-zFedG-T4d",
        "baGDx7g2-Yux",
        "Cc7JWEcj-x2_",
        "C2cMaS1Z-7DW",
        "Q6KKqrKM_sGN",
        "AZ1Le19-_1Cy",
        "UCUWPg17Abt3"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
